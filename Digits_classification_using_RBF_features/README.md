####Lisence:`  If you are using any part of code or algorithm  presented in this code (for your homework), please do not forget to include the source as reference to maintain academic integrity.`How does the digit's image classification works.================================`Please read the homework question paper for problem details.`  First the given data set of 2000 digit images is divided into 1000 Training and 1000 Testing sets. Which is simply done by the following implementation:``      1	Train_vectors =mfeat_pix(1: 100, :); 2	Test_vectors = mfeat_pix(101: 200, :);  3	% fprintf('Splitting data sets to Testing and Training sets......\n');4	for dig = 1:9 5	    raw_digits = mfeat_pix(200*(dig)+1: 100*(2*dig+1), :); 6	    Train_vectors = [Train_vectors;raw_digits];7	    raw_digits = mfeat_pix(200*(dig)+101: 200*(dig+1),:);8	    Test_vectors = [Test_vectors;raw_digits];9	end ``	Then, K *codebook vectors* or *centroids* of clusters is calculated using *K-means clustering*.`[idx, centroid] = kmeans(train_set, K)`The matlab’s inbuilt algorithm is used instead of the algorithm that we implemented because of simplicity, brevity and better time complexity.  Once we have found codebook vectors, following steps are followed:  * **Feature extraction:** We extract the features from the training sets of images with the use of the given formula:  `f(x_i) = exp(-b^(-1) norm(x_i-c_j))`  In other words, at this step, the features of the  vector are going to be extracted with the use of the all codebook vectors  for j = 1,2, …, K. Then matrix X which consists of features as column vectors is computed.`X = [f(x1), f(x2), … , f(xn)]`In order to find the proper decision function, we have to use the **Ridge regression** concept:`W_opt = (XX’+a^2* I_(nxn))^-1  * XY               ...(39)`given by equation (39) from **ML Lecture notes for spring 2017 by Herbert Jaeger.**Where **Y** is our desired output matrix, defined as:  	`1	            Y = zeros(n_test,10);2	            for i = 0:93	                for j = 1 : n_test/104	                    Y(n_test/10*i+j, i+1) = 1;5	                end6	            end`After the computation of decision function `W_opt`,  we use it to make prediction on testing set of images and count the number of miss-classification. The average percentage of misclassification for testing set of images is around **2%**. Cross validation-------------------After understanding the main concept behind the process of the training procedure of the RBF networks. The code is adjusted to include cross validation step in order to find `alpha, beta, K` which yields minimum misclassification.  Here, the whole training set of 1000 images is divided into the `factor = 5` sets. One of those set is considered as validation set and the rest as the training set. In other words, in validation set contains 20 instances of each digit and the rest 80 will be in training set.The training set is used to train the model and validation set is used to test the performance of the trained model. Then, sets of images in the validation set is exchanged with the sets in training set.We subdivide the training dataset into training and validation set using following algorithm:`	1	for i =1:factor2	            %% Divide available training data into Train_Sets and Validation_sets3	            validation_set = zeros(n_validation, 240) ;4	            train_set  = zeros(n_test, 240) ;5	            num = 1000/(10*factor) ;% = 206	            for j = 0:97	                train_set (j * (100-num) +1 : j*(100-num) +(i-1) *num , : )  = digits(j * 100 +1 : j *100 + (i-1)*num, : );8	                validation_set(j * num +1 : (j+1) * num, : ) =  digits(j *100 +(i-1)*num + 1 : j*100+i*num, : );9	                train_set (j * (100-num) +(i-1) * num +1 : (100-num) * (j+1), : )  = digits(j * 100 + i*num +1 :(j+1) *100, : );10	            end11	 12	  %% Rest of the part of code goes here........13	end`Tuning parameters: ----------- A matrix that consisting of  a range of values for these parameters,`alpha, beta & K`  is created and then, inside a loop cross validation step is performed to find the value of `alpha, beta & K` which minimises the number of misclassification.The algorithm presented below is used to tune the parameters:`1	steps  = 10;	2	%Tuning_parameters = [alpha; beta; K];3	K =300; % number of clusters for k means clustering4	K_ul = 800; K_ll = 200; % upper and lower limits for K5	beta = 2000; % 500 - 40006	alpha = 0.1; % 0.08 to 0.16 or 0.127	alpha_ll = 0.05; alpha_ul = 1.16;8	Tuning_range = [alpha_ll : (alpha_ul-alpha_ll)/steps : alpha_ul ; 500: (3000-500)/steps : 3000 ;K_ll: (K_ul-K_ll)/steps : K_ul];9	%%10	for tp = 1:311	    average_error = 1000; % initialization with arbitary max num ----average number of missclassification12	    best_param = Tuning_range (tp,1);13	        14	    for tr = 1: steps+115	        if tp ==1       alpha = Tuning_range(tp,tr);16	            fprintf('\n current alpha = %d\n', alpha);17	        elseif tp ==2   beta = Tuning_range(tp, tr);18	            fprintf('\n current beta = %d\n', beta);19	        elseif tp ==3   K = Tuning_range(tp,tr) ;20	            fprintf('\n current K = %d\n', K);21	        else error('Error 101');22	        end23	%% cross validation goes here24	end`In the given implementation of the code, first the value for `alpha` is tuned by keeping rest of parameters fixed. Then the value of `alpha`  is set with the optimum value and we start tuning `beta` and similarly K in next iteration of loop. The `graph` below shows the performance of decision function with on tuning the parameters.   So the value of these parameters is found to be   	alpha = 0.1610	beta = 2250	K = 500which yields only  **1.9 %** misclassification on validation set and **2.1%** misclassification for training set of 1000 images.**Thus, the given model does the prediction with the accuracy of `97.9%` of accuracy.**